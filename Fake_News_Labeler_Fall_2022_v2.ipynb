{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SukheshNuthalapati/BeeSnap-ML/blob/master/Fake_News_Labeler_Fall_2022_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJQ1vkUS-4hv"
      },
      "source": [
        "# 0. Fake News Labeler\n",
        "\n",
        "This notebook contains code to label the FNC-Core dataset with a team of pre-trained experts. \n",
        "\n",
        "## Dataset\n",
        "\n",
        "The FNC (Fake-News-Covid) dataset is a twitter-sampled stream of tweets for fake news detection.It is collected along with its social context of usernames, account details, retweets, likes, quote tweets, among other features.\n",
        "\n",
        "FNC has several components:\n",
        "\n",
        "- **FNC-Raw**: The raw stream collected from the Twitter Sampled Stream\n",
        "- **FNC-Filtered**: The filtered stream obtained from the Twitter Sampled Stream with covid-related keyword filters.\n",
        "- **FNC-Neighbors**: Samples from FNC-Raw that were not filtered into FNC-Filtered due to missing, censored, or mispelled keywords, but are semantically similar to FNC-Filtered. We detect these with embedding extensions.\n",
        "- **FNC-Extended**: FNC-Filtered combined with FNC-Neighbors to yield a more complete filtered dataset. We can refer to monthly subsets as FNC-Extended-\\<MonthYear\\>, e.g. FNC-Extended-Jan2020\n",
        "\n",
        "These are the unlabeled parts of FNC. With this notebook, we can label a subset of FNC-Extended with high accuracy. These also fall under several components:\n",
        "\n",
        "- **FNC-Extended-Oracle**: Samples from FNC-Extended that are labeled with human annotators. FNC-Extended-Oracle has 10k samples. Since FNC-Extended spans 25 months, we have 400 labeled samples per month.\n",
        "- **FNC-Expert-\\<Expert\\>-\\<MonthYear\\>**: The FNC-Extended-\\<MonthYear\\> subset is labeled by an expert with name \\<Expert\\>.\n",
        "- **FNC-Labeled**: We integrate labels from several experts to generate the final labeled set. We can evaluate labeling accuracy on the intersection with FNC-Extended-Oracle. Monthly subsets are FNC-Labeled-\\<MonthYear\\>\n",
        "\n",
        "## This Notebook\n",
        "In this notebook, we will:\n",
        "\n",
        "- Generate FNC-Filtered (unless the raw data is already of this format...should recheck)\n",
        "- Possibly generate FNC-Neighbors (unless raw data is already of this format...should recheck)\n",
        "- Generate FNC-Expert. For this, we have a list of experts already generated. We will use EdnaML Deployments to load an expert, then load our data from Azure, then label each point, and save the generated label. If a model abstains for some point, we will score that as a [-1]. True News is [1]. Fake News is [0]. Then, the labels will be saved to this instance. We will download the labels, labeled as FNC-Expert-[ExpertName]-[MonthYear]. For each file, we will also save any necessary label characteristics in the same line, i.e.: [label,L-score, model-l-score-threshold, etc]\n",
        "- Possibly Generate FNC-Labeled. Once we have several expert labeled sets, we will integrate them together using Snorkel, EEWS, etc... Snorkel is easy to implement. We might skip EEWS and ATEAM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMDUT5TWE7-N",
        "outputId": "5e81165d-1eed-430d-8694-20a98f08271d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.23.1-py3-none-any.whl (5.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.3 MB 8.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 58.6 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 55.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.9.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.10.1 tokenizers-0.13.1 transformers-4.23.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUETMOy3gGvu"
      },
      "source": [
        "# 1. Setup -- Gdrive Connect, Git clone, Model  Downloads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4h8JjeAfpDo"
      },
      "source": [
        "## Connect to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_P6hPRDqyi4",
        "outputId": "48d81b5f-6f4b-445f-d0a6-0d679f52c4d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZtdWNypnP72"
      },
      "outputs": [],
      "source": [
        "import os.path as osp\n",
        "import os, glob, shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiPHbFAZjWlv",
        "outputId": "64cdeb2a-fb9a-4a0b-8ca5-fb86b22eed12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Set Gogle Drive Connection\n",
        "#if not osp.exists(\"./drive\"):\n",
        "from google.colab import drive\n",
        "\n",
        "#https://stackoverflow.com/questions/69822304/google-colab-google-drive-can%c2%b4t-be-mounted-anymore-browser-popup-google-dri\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqJoyEW1f_a1"
      },
      "source": [
        "## Git Clone"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oisq4kE8IMHi"
      },
      "source": [
        "### From Source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJfycQTLVkEj"
      },
      "outputs": [],
      "source": [
        "! rm -rf -- GLAMOR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goBMKUmagBIx",
        "outputId": "afe3f8a1-b4e0-4982-eb51-3a2811206051"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GLAMOR'...\n",
            "remote: Enumerating objects: 7326, done.\u001b[K\n",
            "remote: Counting objects: 100% (1812/1812), done.\u001b[K\n",
            "remote: Compressing objects: 100% (539/539), done.\u001b[K\n",
            "remote: Total 7326 (delta 1122), reused 1743 (delta 1090), pack-reused 5514\u001b[K\n",
            "Receiving objects: 100% (7326/7326), 2.06 MiB | 13.97 MiB/s, done.\n",
            "Resolving deltas: 100% (4823/4823), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone -b master https://github.com/asuprem/GLAMOR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T80AC-kx4v4Y",
        "outputId": "becd51c1-f4ad-4a79-e5e0-b06ce187d738"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/GLAMOR\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from ednaml==0.1.5) (1.0.2)\n",
            "Requirement already satisfied: torch>=1.10.* in /usr/local/lib/python3.7/dist-packages (from ednaml==0.1.5) (1.12.1+cu113)\n",
            "Collecting torchinfo>=1.6.5\n",
            "  Downloading torchinfo-1.7.1-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: torchvision>=0.11.* in /usr/local/lib/python3.7/dist-packages (from ednaml==0.1.5) (0.13.1+cu113)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from ednaml==0.1.5) (7.1.2)\n",
            "Requirement already satisfied: tqdm>=4.63.* in /usr/local/lib/python3.7/dist-packages (from ednaml==0.1.5) (4.64.1)\n",
            "Collecting sentencepiece>=0.1.96\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 9.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from ednaml==0.1.5) (2.4.0)\n",
            "Requirement already satisfied: pyyaml>=6.0 in /usr/local/lib/python3.7/dist-packages (from ednaml==0.1.5) (6.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.2->ednaml==0.1.5) (1.21.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.2->ednaml==0.1.5) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.2->ednaml==0.1.5) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.2->ednaml==0.1.5) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.10.*->ednaml==0.1.5) (4.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.11.*->ednaml==0.1.5) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.11.*->ednaml==0.1.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.11.*->ednaml==0.1.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.11.*->ednaml==0.1.5) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.11.*->ednaml==0.1.5) (3.0.4)\n",
            "Installing collected packages: torchinfo, sentencepiece, ednaml\n",
            "  Running setup.py develop for ednaml\n",
            "Successfully installed ednaml-0.1.5 sentencepiece-0.1.97 torchinfo-1.7.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -e GLAMOR/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5j3WfN0fpIT"
      },
      "source": [
        "###  From PyPi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7dkOhZi08dU"
      },
      "outputs": [],
      "source": [
        "#! python -V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwqgjiZ331ik"
      },
      "outputs": [],
      "source": [
        "#! pip3 install --pre ednaml==0.1.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQCbxSJDf7Ix"
      },
      "source": [
        "## AlBERT / ModelFile Data Download\n",
        "\n",
        "(And assume they are FNC-Extended-MonthYear)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27LK1AmgfuCk"
      },
      "outputs": [],
      "source": [
        "!cp ./drive/MyDrive/Datasets/PreTrained/Albertv2/30k* .\n",
        "!cp ./drive/MyDrive/Datasets/PreTrained/Albertv2/pytorch_model.bin ."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Restart Runtime Here!!!!"
      ],
      "metadata": {
        "id": "TfspmIFRmgd6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KjvT_ZJovGY"
      },
      "source": [
        "# 2. Raw Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoQKEy4Douez"
      },
      "outputs": [],
      "source": [
        "tweet_file = \"tweets-2020-01-22\"\n",
        "proxy = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqCdOF7tCj8N"
      },
      "source": [
        "# 3. Generating FNC-Filtered from FNC-Raw\n",
        "\n",
        "Here, we will use an EdnaML workflow to generate FNC-Filtered from FNC-Raw. This is not really a learnable model, rather a simple keyword matcher.\n",
        "\n",
        "Essentially, we will build a workflow where we:\\\n",
        "\n",
        "1. Crawl an FNC Source and download if it does not exist\n",
        "2. Create datashards from the Source, with only testing data since there is no \"training\" to be performed\n",
        "3. Deploy a ModelAbstract model that reads a batch of raw text data, and checks whether elements of this batch contain our keywords. If they do, we label 1. If they do not, we label 0. \n",
        "4. The Deployment thus generates 2 outputs: a file containing fnc-filtered, and a file fnc-nonfiltered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEwLZP2RnPe3"
      },
      "outputs": [],
      "source": [
        "# cleanup\n",
        "!rm -rf -- test-datashard-artifacts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbgIkVzUi8RM"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "upU2x5V1CFqi",
        "outputId": "53bd85aa-40f3-4a31-f4e7-35952e5327d8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.12.1+cu113'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import torch\n",
        "import ednaml\n",
        "import glob, os\n",
        "from ednaml.core import EdnaDeploy, EdnaML\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fj4K8740CFoI"
      },
      "outputs": [],
      "source": [
        "config = \"./GLAMOR/profiles/FNC/filtered/config.yml\"\n",
        "crawler_generator = \"./GLAMOR/profiles/FNC/fnc-crawler.py\"\n",
        "filter_model_deployment = \"./GLAMOR/profiles/FNC/fnc-filtered.py\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lstv_fo4CFlo"
      },
      "outputs": [],
      "source": [
        "ed = EdnaDeploy(config=config)\n",
        "ed.cfg.DEPLOYMENT.OUTPUT_ARGS[\"basename\"] = \"%s\"%tweet_file\n",
        "ed.cfg.DEPLOYMENT.DATAREADER.CRAWLER_ARGS[\"azfile\"] = \"%s.json.gz\"%tweet_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZNRsfX7HAGj",
        "outputId": "ae09025a-ac80-4e07-caee-25b19fa80912"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "04:30:36 Adding a crawler, from /content/GLAMOR/profiles/FNC/fnc-crawler.py, with inferred name FNCCrawler\n",
            "04:30:36 Adding a generator, from /content/GLAMOR/profiles/FNC/fnc-crawler.py, with inferred name FNCRawGenerator\n",
            "04:30:36 Adding a model, from /content/GLAMOR/profiles/FNC/fnc-filtered.py, with inferred name FNCFilter\n",
            "04:30:36 Adding a deployment, from /content/GLAMOR/profiles/FNC/fnc-filtered.py, with inferred name FNCFilterDeployment\n",
            "04:30:36 ****************************************\n",
            "04:30:36 \n",
            "04:30:36 \n",
            "04:30:36 Using the following configuration:\n",
            "04:30:36 DEPLOYMENT:\n",
            "  DATAREADER:\n",
            "    CRAWLER_ARGS:\n",
            "      azcontainer: edna-covid-raw\n",
            "      azfile: tweets-2020-01-22.json.gz\n",
            "      azstorage: ednadatasets\n",
            "    DATAREADER: AlbertReader\n",
            "    DATASET_ARGS:\n",
            "      classificationclass:\n",
            "      - fnews\n",
            "      shard_replace: false\n",
            "      shardname: fnc-raw-shard\n",
            "      shardpath: datashard-artifacts\n",
            "      shardsize: 20000\n",
            "      shuffle: false\n",
            "    GENERATOR: null\n",
            "    GENERATOR_ARGS: {}\n",
            "  DEPLOY: BaseDeploy\n",
            "  DEPLOYMENT_ARGS: {}\n",
            "  EPOCHS: 1\n",
            "  OUTPUT_ARGS:\n",
            "    basename: tweets-2020-01-22\n",
            "    filtered_output: filtered\n",
            "    unfiltered_output: unfiltered\n",
            "  PLUGIN:\n",
            "    HOOKS: always\n",
            "    RESET: false\n",
            "EXECUTION:\n",
            "  DATAREADER:\n",
            "    CRAWLER_ARGS: {}\n",
            "    DATAREADER: DataReader\n",
            "    DATASET_ARGS: {}\n",
            "    GENERATOR: null\n",
            "    GENERATOR_ARGS: {}\n",
            "  EPOCHS: 10\n",
            "  FP16: false\n",
            "  MODEL_SERVING: false\n",
            "  OPTIMIZER_BUILDER: ClassificationOptimizer\n",
            "  PLUGIN:\n",
            "    HOOKS: always\n",
            "    RESET: false\n",
            "  SKIPEVAL: false\n",
            "  TEST_FREQUENCY: 1\n",
            "  TRAINER: BaseTrainer\n",
            "  TRAINER_ARGS:\n",
            "    accumulation_steps: 1\n",
            "LOGGING:\n",
            "  INPUT_SIZE: null\n",
            "  STEP_VERBOSE: 100\n",
            "LOSS: []\n",
            "LOSS_OPTIMIZER:\n",
            "- BASE_LR: 0.001\n",
            "  LR_BIAS_FACTOR: 1.0\n",
            "  OPTIMIZER: Adam\n",
            "  OPTIMIZER_KWARGS: {}\n",
            "  OPTIMIZER_NAME: opt-1\n",
            "  WEIGHT_BIAS_FACTOR: 0.0005\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "LOSS_SCHEDULER:\n",
            "- LR_KWARGS:\n",
            "    step_size: 20\n",
            "  LR_SCHEDULER: StepLR\n",
            "  SCHEDULER_NAME: opt-1\n",
            "MODEL:\n",
            "  BUILDER: ednaml_model_builder\n",
            "  MODEL_ARCH: FNCFilter\n",
            "  MODEL_BASE: NoBase\n",
            "  MODEL_KWARGS:\n",
            "    filter_list:\n",
            "    - covid\n",
            "    - corona\n",
            "    - mask\n",
            "    - wuhan\n",
            "    - n95\n",
            "    - sars\n",
            "    - monkey\n",
            "    - pandemic\n",
            "    - social\n",
            "    - quarantin\n",
            "    - virus\n",
            "    - infect\n",
            "    - lock\n",
            "    - ppe\n",
            "    - variant\n",
            "    - vaccine\n",
            "    - travel\n",
            "    - omicron\n",
            "    - ivermectin\n",
            "    - plandemic\n",
            "    - 5g\n",
            "    - gates\n",
            "    - hoax\n",
            "    - bioweapon\n",
            "    - bat\n",
            "    - fauci\n",
            "  MODEL_NORMALIZATION: bn\n",
            "  PARAMETER_GROUPS:\n",
            "  - opt-1\n",
            "MODEL_PLUGIN: {}\n",
            "OPTIMIZER:\n",
            "- BASE_LR: 0.001\n",
            "  LR_BIAS_FACTOR: 1.0\n",
            "  OPTIMIZER: Adam\n",
            "  OPTIMIZER_KWARGS: {}\n",
            "  OPTIMIZER_NAME: opt-1\n",
            "  WEIGHT_BIAS_FACTOR: 0.0005\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "SAVE:\n",
            "  CHECKPOINT_DIRECTORY: ./drive/MyDrive/Projects/FNC/Models/\n",
            "  DRIVE_BACKUP: false\n",
            "  LOG_BACKUP: false\n",
            "  MODEL_BACKBONE: offshelf\n",
            "  MODEL_CORE_NAME: fnc-filter\n",
            "  MODEL_QUALIFIER: raw\n",
            "  MODEL_VERSION: 1\n",
            "  SAVE_FREQUENCY: 5\n",
            "SCHEDULER:\n",
            "- LR_KWARGS:\n",
            "    step_size: 20\n",
            "  LR_SCHEDULER: StepLR\n",
            "  SCHEDULER_NAME: opt-1\n",
            "STORAGE:\n",
            "  STORAGE_ARGS: {}\n",
            "  TYPE: BaseStorage\n",
            "  URL: ./\n",
            "TEST_TRANSFORMATION:\n",
            "  ARGS: {}\n",
            "  BATCH_SIZE: 128\n",
            "  WORKERS: 2\n",
            "TRAIN_TRANSFORMATION:\n",
            "  ARGS: {}\n",
            "  BATCH_SIZE: 128\n",
            "  WORKERS: 2\n",
            "extensions:\n",
            "- EXECUTION\n",
            "- SAVE\n",
            "- STORAGE\n",
            "- TRANSFORMATION\n",
            "- MODEL\n",
            "- LOSS\n",
            "- OPTIMIZER\n",
            "- SCHEDULER\n",
            "- LOSS_OPTIMIZER\n",
            "- LOSS_SCHEDULER\n",
            "- LOGGING\n",
            "- DEPLOYMENT\n",
            "- MODEL_PLUGIN\n",
            "\n",
            "04:30:36 \n",
            "04:30:36 \n",
            "04:30:36 ****************************************\n",
            "/content/GLAMOR/src/ednaml/core/EdnaML.py:220: UserWarning: Mode is `test` but weights is `None`. This will cause issues when EdnaML attempts to load weights\n",
            "  \"Mode is `test` but weights is `None`. This will cause\"\n",
            "04:30:36 No previous stop detected. Will start from epoch 0\n",
            "04:30:36 Reading data with DataReader AlbertReader\n",
            "04:30:36 Default CRAWLER is <class 'ednaml.crawlers.Crawler'>\n",
            "04:30:36 Default DATASET is <class 'ednaml.generators.AlbertGenerator.AlbertDataset'>\n",
            "04:30:36 Default GENERATOR is <class 'ednaml.generators.AlbertGenerator.AlbertGenerator'>\n",
            "04:30:36 Updating GENERATOR to queued class FNCRawGenerator\n",
            "04:30:36 Updating CRAWLER to FNCCrawler\n",
            "04:30:36 Crawling https://ednadatasets.blob.core.windows.net/edna-covid-raw/tweets-2020-01-22.json.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Registering a crawler: <class './GLAMOR/profiles/FNC/fnc-crawler.py.FNCCrawler'>, from file: /content/GLAMOR/profiles/FNC/fnc-crawler.py\n",
            "Registering a generator: <class './GLAMOR/profiles/FNC/fnc-crawler.py.FNCRawGenerator'>, from file: /content/GLAMOR/profiles/FNC/fnc-crawler.py\n",
            "Registering a model: <class './GLAMOR/profiles/FNC/fnc-filtered.py.FNCFilter'>, from file: /content/GLAMOR/profiles/FNC/fnc-filtered.py\n",
            "Registering a deployment: <class './GLAMOR/profiles/FNC/fnc-filtered.py.FNCFilterDeployment'>, from file: /content/GLAMOR/profiles/FNC/fnc-filtered.py\n",
            "5637377/5637377 bytes [████████████████████████████████████████████████████████████████████████████████████████████████████]\n",
            "Download of tweets-2020-01-22.json.gz to https://ednadatasets.blob.core.windows.net/edna-covid-raw/tweets-2020-01-22.json.gz completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "04:30:39 Generating dataloader `FNCRawGenerator` with `test` mode\n",
            "04:30:47 Generated test data/query generator\n",
            "04:30:47 Loaded ednaml_model_builder from ednaml.models to build model\n",
            "04:30:47 Finished instantiating model with FNCFilter architecture\n",
            "04:30:47 Adding plugins after constructing model\n",
            "04:30:47 No saved model weights provided. Inferring weights path.\n",
            "04:30:47 No previous stop exists. Not loading weights.\n",
            "04:30:47 Model Summary retured the following error:\n",
            "04:30:47 Traceback (most recent call last):\n",
            "  File \"/content/GLAMOR/src/ednaml/core/EdnaML.py\", line 875, in getModelSummary\n",
            "    self.cfg.TRAIN_TRANSFORMATION.INPUT_SIZE,\n",
            "AttributeError: 'TransformationConfig' object has no attribute 'INPUT_SIZE'\n",
            "\n",
            "04:30:47 Multi-gpu or non-gpu not yet fully supported.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NOT saving metadata. saveMetadata() function not set up.\n"
          ]
        }
      ],
      "source": [
        "ed.add(crawler_generator)\n",
        "ed.add(filter_model_deployment)\n",
        "ed.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkWzzI6XCFjM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8a69e9c-70fa-45e1-f30c-8bacc437e07e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "04:30:47 Starting deployment\n",
            "04:30:47 Logging to:\tfnc-filter-v1-offshelf-raw-logger.log\n",
            "04:30:47 Setting up plugin hooks. Plugins will fire during:  always\n",
            "04:30:47 Executing deployment for  1 epochs\n",
            "04:30:47 Starting epoch 0\n",
            "04:30:49 Executing end of epoch steps\n",
            "04:30:49 Completed deployment task.\n"
          ]
        }
      ],
      "source": [
        "ed.deploy()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleanup"
      ],
      "metadata": {
        "id": "FOwc42Dv5i6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if os.path.exists(tweet_file+\".json\"):\n",
        "  os.remove(tweet_file+\".json\")\n",
        "if os.path.exists(tweet_file+\".json.gz\"):\n",
        "  os.remove(tweet_file+\".json.gz\")\n",
        "! rm test-datashard-artifacts/fnc-raw*\n",
        "! rm train-datashard-artifacts/fnc-raw*\n",
        "! rm test-datashard-artifacts/len.txt"
      ],
      "metadata": {
        "id": "1grPg6X_5giI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dckc897vD9t_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88b3de7a-5e06-47da-d4bb-72d7f9ffdfbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17718 tweets-2020-01-22-filtered.json\n"
          ]
        }
      ],
      "source": [
        "! wc -l $tweet_file-filtered.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! wc -l $tweet_file-unfiltered.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idvTSx5L4rSV",
        "outputId": "07bed0f5-a5ac-4b1f-a44a-7150512acb83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6333 tweets-2020-01-22-unfiltered.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filtered subset scaling (for class)"
      ],
      "metadata": {
        "id": "bBLksdNI5Ukb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filter_scale = 1    # For Caden, Sneh --> change this to 0.8\n",
        "unfilter_scale = 1  # For Caden, Sneh --> change this to 0.6"
      ],
      "metadata": {
        "id": "Z6wqZBNo5YtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scaling"
      ],
      "metadata": {
        "id": "kqSK-TBM5Z9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "tf_filtered = tweet_file + \"-filtered.json\"\n",
        "tf_unfiltered = tweet_file + \"-unfiltered.json\"\n",
        "# Set reduction scale\n",
        "import random\n",
        "random.seed(34598344)\n",
        "# Reduce filtered.json\n",
        "filt_obj = open(tf_filtered, \"r\")\n",
        "filtout_obj = open(tf_filtered + \"2\", \"w\")\n",
        "for row in filt_obj:\n",
        "    if random.random() <= filter_scale:\n",
        "      filtout_obj.write(row)\n",
        "filtout_obj.close()\n",
        "filt_obj.close()\n",
        "# Reduce unfiltered.json\n",
        "unfilt_obj = open(tf_unfiltered, \"r\")\n",
        "unfiltout_obj = open(tf_unfiltered + \"2\", \"w\")\n",
        "for row in unfilt_obj:\n",
        "    if random.random() <= unfilter_scale:\n",
        "      unfiltout_obj.write(row)\n",
        "unfiltout_obj.close()\n",
        "unfilt_obj.close()\n",
        "# Delete old files\n",
        "os.remove(tf_filtered)\n",
        "os.remove(tf_unfiltered)\n",
        "# Rename to old file name\n",
        "os.rename(tf_filtered+\"2\", tf_filtered)\n",
        "os.rename(tf_unfiltered+\"2\", tf_unfiltered)"
      ],
      "metadata": {
        "id": "kmSTGx8U47rY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88b3de7a-5e06-47da-d4bb-72d7f9ffdfbb",
        "id": "NItLF9wU5EID"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17718 tweets-2020-01-22-filtered.json\n"
          ]
        }
      ],
      "source": [
        "! wc -l $tweet_file-filtered.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! wc -l $tweet_file-unfiltered.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07bed0f5-a5ac-4b1f-a44a-7150512acb83",
        "id": "aIgD01x55EIE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6333 tweets-2020-01-22-unfiltered.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDG0xKuTCrT5"
      },
      "source": [
        "# 4. Generating FNC-Neighbors from FNC-Filtered and FNC\n",
        "\n",
        "Here, we will take the json files generated from FNC-Filter: `filter.json` and `unfilter.json`.\n",
        "\n",
        "The filter.json content is used to fine-tune a BERT model. This BERT model implements keyword masking, that is, specific keywords are masked\n",
        "\n",
        "Then we use a cluster_estimator plugin to estimate the best number of clusters with KMeans for the training dataset.\n",
        "\n",
        "Then, we insert a high-density estimator plugin with the best number of clusters as proxies\n",
        "\n",
        "Then, we will use a Deployment to iterate through unfilter. For each, we will generate the features, and use the high-density estimator plugin to check if it exists in the high density set of existing clusters. Note that the unfilter data implements the same masking as the training model. Any samples that exist are placed in `extended.json`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUiGLd_6sbPG"
      },
      "source": [
        "## 4.1 Finetuning MLM with keyword masking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOGkQT_yJPKq"
      },
      "outputs": [],
      "source": [
        "# Cleanup ONLY if starting from scratch, not if already doing 4.1\n",
        "# Also cleanup if switching datasets\n",
        "!rm test-datashard-artifacts/fnc-filtermask*\n",
        "!rm train-datashard-artifacts/fnc-filtermask*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbr7XsGK_ega",
        "outputId": "da4a1a04-0cea-412f-f388-e7530af46875"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "v38XZXs-_egb",
        "outputId": "9b8abebf-c852-495c-cff0-24ee79ffe3f7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.12.1+cu113'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import torch\n",
        "import ednaml\n",
        "import glob, os\n",
        "from ednaml.core import EdnaDeploy, EdnaML\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKtTKmW__egb"
      },
      "outputs": [],
      "source": [
        "config = \"./GLAMOR/profiles/FNC/extension/config.yml\"\n",
        "crawler = \"./GLAMOR/profiles/FNC/fnc-crawler.py\"\n",
        "filtermask_generator = \"./GLAMOR/profiles/FNC/fnc-filtermasked.py\"\n",
        "extension_model = \"./GLAMOR/profiles/FNC/fnc-extension.py\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WbCacu9_egb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ecd74a1-321d-4926-8c30-ed6544bcca0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Injected key-value pair:  SAVE.MODEL_QUALIFIER, tweets-2020-01-22\n",
            "Injected key-value pair:  SAVE.MODEL_VERSION, 4\n",
            "Log file exists at fnc-extension-v4-albert-tweets-2020-01-22/fnc-extension-v4-albert-tweets-2020-01-22-logger.log. Will attempt to append there.\n"
          ]
        }
      ],
      "source": [
        "eml = EdnaML(config=config, config_inject=[(\"SAVE.MODEL_QUALIFIER\", tweet_file), (\"SAVE.MODEL_VERSION\", 1), (\"SAVE.STEP_SAVE_FREQUENCY\", 50000), (\"EXECUTION.EPOCHS\", 10)])\n",
        "#eml = EdnaML(config=config, config_inject=[(\"SAVE.MODEL_QUALIFIER\", tweet_file)])\n",
        "eml.cfg.EXECUTION.DATAREADER.CRAWLER_ARGS[\"train_file\"] = \"%s-filtered.json\"%tweet_file\n",
        "eml.cfg.EXECUTION.DATAREADER.CRAWLER_ARGS[\"test_file\"] = \"%s-unfiltered.json\"%tweet_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_l1QmVo6_egb",
        "outputId": "4934c9f9-2721-4005-a1e0-dbe394023ddf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "19:49:02 Adding a crawler, from /content/GLAMOR/profiles/FNC/fnc-crawler.py, with inferred name FNCCrawler\n",
            "19:49:02 Adding a generator, from /content/GLAMOR/profiles/FNC/fnc-crawler.py, with inferred name FNCRawGenerator\n",
            "19:49:02 Adding a generator, from /content/GLAMOR/profiles/FNC/fnc-filtermasked.py, with inferred name FNCFilterMaskGenerator\n",
            "19:49:02 Adding a model, from /content/GLAMOR/profiles/FNC/fnc-extension.py, with inferred name FNCAlbertModeler\n",
            "19:49:02 Adding a trainer, from /content/GLAMOR/profiles/FNC/fnc-extension.py, with inferred name FNCAlbertModelerTrainer\n",
            "19:49:02 ****************************************\n",
            "19:49:02 \n",
            "19:49:02 \n",
            "19:49:02 Using the following configuration:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Registering a crawler: <class './GLAMOR/profiles/FNC/fnc-crawler.py.FNCCrawler'>, from file: /content/GLAMOR/profiles/FNC/fnc-crawler.py\n",
            "Registering a generator: <class './GLAMOR/profiles/FNC/fnc-crawler.py.FNCRawGenerator'>, from file: /content/GLAMOR/profiles/FNC/fnc-crawler.py\n",
            "Registering a generator: <class './GLAMOR/profiles/FNC/fnc-filtermasked.py.FNCFilterMaskGenerator'>, from file: /content/GLAMOR/profiles/FNC/fnc-filtermasked.py\n",
            "Registering a model: <class './GLAMOR/profiles/FNC/fnc-extension.py.FNCAlbertModeler'>, from file: /content/GLAMOR/profiles/FNC/fnc-extension.py\n",
            "Registering a trainer: <class './GLAMOR/profiles/FNC/fnc-extension.py.FNCAlbertModelerTrainer'>, from file: /content/GLAMOR/profiles/FNC/fnc-extension.py\n",
            "Registering a deployment: <class './GLAMOR/profiles/FNC/fnc-extension.py.FNCTrainingFeaturesDeploy'>, from file: /content/GLAMOR/profiles/FNC/fnc-extension.py\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "19:49:02 DEPLOYMENT:\n",
            "  DATAREADER:\n",
            "    CRAWLER_ARGS: {}\n",
            "    DATAREADER: DataReader\n",
            "    DATASET_ARGS: {}\n",
            "    GENERATOR: null\n",
            "    GENERATOR_ARGS: {}\n",
            "  DEPLOY: BaseDeploy\n",
            "  DEPLOYMENT_ARGS: {}\n",
            "  EPOCHS: 1\n",
            "  OUTPUT_ARGS: {}\n",
            "  PLUGIN:\n",
            "    HOOKS: always\n",
            "    RESET: false\n",
            "EXECUTION:\n",
            "  DATAREADER:\n",
            "    CRAWLER_ARGS:\n",
            "      azcontainer: edna-covid-raw\n",
            "      azfile: tweets-2020-01-22.json.gz\n",
            "      azstorage: ednadatasets\n",
            "      test_file: tweets-2020-01-22-unfiltered.json\n",
            "      train_file: tweets-2020-01-22-filtered.json\n",
            "    DATAREADER: AlbertReader\n",
            "    DATASET_ARGS:\n",
            "      classificationclass:\n",
            "      - fnews\n",
            "      data_shuffle: true\n",
            "      keyword_mask: true\n",
            "      keywords:\n",
            "      - covid\n",
            "      - corona\n",
            "      - mask\n",
            "      - wuhan\n",
            "      - n95\n",
            "      - sars\n",
            "      - monkey\n",
            "      - pandemic\n",
            "      - social\n",
            "      - quarantin\n",
            "      - virus\n",
            "      - infect\n",
            "      - lock\n",
            "      - ppe\n",
            "      - variant\n",
            "      - vaccine\n",
            "      - travel\n",
            "      - omicron\n",
            "      - ivermectin\n",
            "      - plandemic\n",
            "      - 5g\n",
            "      - gates\n",
            "      - hoax\n",
            "      - bioweapon\n",
            "      - bat\n",
            "      - fauci\n",
            "      masking: false\n",
            "      maxlen: 512\n",
            "      mlm_probability: 0.15\n",
            "      shard_replace: false\n",
            "      shardcache: true\n",
            "      shardname: fnc-filtermask-shard\n",
            "      shardpath: datashard-artifacts\n",
            "      shardsize: 20000\n",
            "      shuffle: true\n",
            "    GENERATOR: null\n",
            "    GENERATOR_ARGS:\n",
            "      from_pretrained: albert-base-v2\n",
            "      tokenizer: HFAutoTokenizer\n",
            "  EPOCHS: 3\n",
            "  FP16: false\n",
            "  MODEL_SERVING: false\n",
            "  OPTIMIZER_BUILDER: ClassificationOptimizer\n",
            "  PLUGIN:\n",
            "    HOOKS: always\n",
            "    RESET: false\n",
            "  SKIPEVAL: false\n",
            "  TEST_FREQUENCY: 1\n",
            "  TRAINER: BaseTrainer\n",
            "  TRAINER_ARGS:\n",
            "    accumulation_steps: 8\n",
            "LOGGING:\n",
            "  INPUT_SIZE:\n",
            "  - 16\n",
            "  - 512\n",
            "  STEP_VERBOSE: 100\n",
            "LOSS:\n",
            "- KWARGS:\n",
            "  - loss_class: CrossEntropyLoss\n",
            "    loss_kwargs:\n",
            "      ignore_index: -1\n",
            "  LABEL: mask_lm\n",
            "  LAMBDAS:\n",
            "  - 1.0\n",
            "  LOSSES:\n",
            "  - TorchLoss\n",
            "  NAME: mask_lm\n",
            "LOSS_OPTIMIZER:\n",
            "- BASE_LR: 0.001\n",
            "  LR_BIAS_FACTOR: 1.0\n",
            "  OPTIMIZER: Adam\n",
            "  OPTIMIZER_KWARGS: {}\n",
            "  OPTIMIZER_NAME: opt-1\n",
            "  WEIGHT_BIAS_FACTOR: 0.0005\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "LOSS_SCHEDULER:\n",
            "- LR_KWARGS:\n",
            "    step_size: 20\n",
            "  LR_SCHEDULER: StepLR\n",
            "  SCHEDULER_NAME: opt-1\n",
            "MODEL:\n",
            "  BUILDER: ednaml_model_builder\n",
            "  MODEL_ARCH: FNCAlbertModeler\n",
            "  MODEL_BASE: Albert\n",
            "  MODEL_KWARGS:\n",
            "    attention_probs_dropout_prob: 0\n",
            "    embedding_size: 128\n",
            "    hidden_act: gelu\n",
            "    hidden_dropout_prob: 0\n",
            "    hidden_size: 768\n",
            "    initializer_range: 0.02\n",
            "    inner_group_num: 1\n",
            "    intermediate_size: 3072\n",
            "    layer_norm_eps: 1.0e-12\n",
            "    max_position_embeddings: 512\n",
            "    num_attention_heads: 12\n",
            "    num_hidden_groups: 1\n",
            "    num_hidden_layers: 12\n",
            "    pooling: pooled\n",
            "    type_vocab_size: 2\n",
            "    vocab_size_or_config_json_file: 30000\n",
            "  MODEL_NORMALIZATION: bn\n",
            "  PARAMETER_GROUPS:\n",
            "  - opt-1\n",
            "MODEL_PLUGIN: {}\n",
            "OPTIMIZER:\n",
            "- BASE_LR: 1.0e-05\n",
            "  LR_BIAS_FACTOR: 1.0\n",
            "  OPTIMIZER: AdamW\n",
            "  OPTIMIZER_KWARGS:\n",
            "    eps: 1.0e-06\n",
            "  OPTIMIZER_NAME: opt-1\n",
            "  WEIGHT_BIAS_FACTOR: 0.0005\n",
            "  WEIGHT_DECAY: 0.0005\n",
            "SAVE:\n",
            "  CHECKPOINT_DIRECTORY: ./drive/MyDrive/Projects/FNC/Models/\n",
            "  DRIVE_BACKUP: true\n",
            "  LOG_BACKUP: false\n",
            "  MODEL_BACKBONE: albert\n",
            "  MODEL_CORE_NAME: fnc-extension\n",
            "  MODEL_QUALIFIER: tweets-2020-01-22\n",
            "  MODEL_VERSION: 4\n",
            "  SAVE_FREQUENCY: 1\n",
            "SCHEDULER:\n",
            "- LR_KWARGS:\n",
            "    gamma: 0.5\n",
            "    step_size: 5\n",
            "  LR_SCHEDULER: StepLR\n",
            "  SCHEDULER_NAME: opt-1\n",
            "STORAGE:\n",
            "  STORAGE_ARGS: {}\n",
            "  TYPE: BaseStorage\n",
            "  URL: ./\n",
            "TEST_TRANSFORMATION:\n",
            "  ARGS: {}\n",
            "  BATCH_SIZE: 16\n",
            "  WORKERS: 2\n",
            "TRAIN_TRANSFORMATION:\n",
            "  ARGS: {}\n",
            "  BATCH_SIZE: 16\n",
            "  WORKERS: 2\n",
            "extensions:\n",
            "- EXECUTION\n",
            "- SAVE\n",
            "- STORAGE\n",
            "- TRANSFORMATION\n",
            "- MODEL\n",
            "- LOSS\n",
            "- OPTIMIZER\n",
            "- SCHEDULER\n",
            "- LOSS_OPTIMIZER\n",
            "- LOSS_SCHEDULER\n",
            "- LOGGING\n",
            "- DEPLOYMENT\n",
            "- MODEL_PLUGIN\n",
            "\n",
            "19:49:02 \n",
            "19:49:02 \n",
            "19:49:02 ****************************************\n",
            "19:49:02 Previous stop detected. Will attempt to resume from epoch 0\n",
            "19:49:02 Loaded BaseStorage from ednaml.storage to build Storage\n",
            "19:49:02 Reading data with DataReader AlbertReader\n",
            "19:49:02 Default CRAWLER is <class 'ednaml.crawlers.Crawler'>\n",
            "19:49:02 Default DATASET is <class 'ednaml.generators.AlbertGenerator.AlbertDataset'>\n",
            "19:49:02 Default GENERATOR is <class 'ednaml.generators.AlbertGenerator.AlbertGenerator'>\n",
            "19:49:02 Updating GENERATOR to queued class FNCFilterMaskGenerator\n",
            "19:49:02 Updating CRAWLER to FNCCrawler\n",
            "19:49:02 Building Transforms\n",
            "19:49:03 Building Dataset\n",
            "19:49:03 Generating shards\n",
            "100%|██████████| 1/1 [00:11<00:00, 11.56s/it]\n",
            "19:49:15 Building Dataloader\n",
            "19:49:15 Generated training data generator with 17718 training data points\n",
            "19:49:15 Running classification model with classes: {'fnews': {'classes': 2}}\n",
            "19:49:15 Building Transforms\n",
            "19:49:17 Building Dataset\n",
            "19:49:17 Generating shards\n",
            "100%|██████████| 1/1 [00:03<00:00,  3.43s/it]\n",
            "19:49:20 Building Dataloader\n",
            "19:49:20 Generated test data/query generator\n",
            "19:49:20 Loaded ednaml_model_builder from ednaml.models to build model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading weights file pytorch_model.bin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "19:49:20 Finished instantiating model with FNCAlbertModeler architecture\n",
            "19:49:20 Adding plugins after constructing model\n",
            "19:49:20 No saved model weights provided.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Errors \n",
            "\t {'missing_keys': [], 'unexpected_keys': [], 'error_msgs': []}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "19:49:22 Model Summary retured the following error:\n",
            "19:49:22 Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torchinfo/torchinfo.py\", line 287, in forward_pass\n",
            "    _ = model.to(device)(*x, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1148, in _call_impl\n",
            "    result = forward_call(*input, **kwargs)\n",
            "  File \"/content/GLAMOR/src/ednaml/models/ModelAbstract.py\", line 141, in forward\n",
            "    x, **kwargs\n",
            "  File \"./GLAMOR/profiles/FNC/fnc-extension.py\", line 110, in forward_impl\n",
            "    head_mask=head_mask,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1148, in _call_impl\n",
            "    result = forward_call(*input, **kwargs)\n",
            "  File \"/content/GLAMOR/src/ednaml/models/Albert.py\", line 706, in forward\n",
            "    input_ids, position_ids=position_ids, token_type_ids=token_type_ids\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1148, in _call_impl\n",
            "    result = forward_call(*input, **kwargs)\n",
            "  File \"/content/GLAMOR/src/ednaml/models/Albert.py\", line 140, in forward\n",
            "    words_embeddings = self.word_embeddings(input_ids)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1148, in _call_impl\n",
            "    result = forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/sparse.py\", line 160, in forward\n",
            "    self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\", line 2199, in embedding\n",
            "    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n",
            "RuntimeError: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/GLAMOR/src/ednaml/core/EdnaML.py\", line 892, in getModelSummary\n",
            "    dtypes=dtypes,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torchinfo/torchinfo.py\", line 218, in summary\n",
            "    model, x, batch_dim, cache_forward_pass, device, model_mode, **kwargs\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torchinfo/torchinfo.py\", line 299, in forward_pass\n",
            "    ) from e\n",
            "RuntimeError: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: []\n",
            "\n",
            "19:49:22 Loaded ClassificationOptimizer from ednaml.optimizer to build Optimizer model\n",
            "19:49:22 Built optimizer\n",
            "19:49:22 Built scheduler\n",
            "19:49:22 Added TorchLoss with lambda = 1.0 and loss arguments {'loss_class': 'CrossEntropyLoss', 'loss_kwargs': {'ignore_index': -1}}\n",
            "19:49:22 Built loss function\n",
            "19:49:22 Built loss optimizer\n",
            "19:49:22 Built loss scheduler\n",
            "19:49:22 Loaded BaseStorage from ednaml.storage to build Storage\n",
            "19:49:22 1 GPUs available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INPUT SIZE ====  [16, 512]\n",
            "NOT saving metadata. saveMetadata() function not set up.\n"
          ]
        }
      ],
      "source": [
        "eml.add(crawler)\n",
        "eml.add(filtermask_generator)\n",
        "eml.add(extension_model)\n",
        "eml.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        },
        "id": "hf5tx5mF_egb",
        "outputId": "70e47022-cc4a-4e23-9e02-f88283b1eefb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "19:49:22 Starting training\n",
            "19:49:22 Logging to:\tfnc-extension-v4-albert-tweets-2020-01-22-logger.log\n",
            "19:49:22 Models will be saved to local directory:\tfnc-extension-v4-albert-tweets-2020-01-22\n",
            "19:49:22 Models will be backed up to drive directory:\t./drive/MyDrive/Projects/FNC/Models/fnc-extension-v4-albert-tweets-2020-01-22\n",
            "19:49:22 Models will be saved with base name:\tfnc-extension-v4_epoch[].pth\n",
            "19:49:22 Optimizers will be saved with base name:\tfnc-extension-v4_epoch[]_optimizer.pth\n",
            "19:49:22 Schedulers will be saved with base name:\tfnc-extension-v4_epoch[]_scheduler.pth\n",
            "19:49:22 Resuming training from epoch 1. Loading saved state from 0\n",
            "19:49:22 Loading model, optimizer, and scheduler from drive backup.\n",
            "19:49:22 Finished loading model state_dict from ./drive/MyDrive/Projects/FNC/Models/fnc-extension-v4-albert-tweets-2020-01-22/fnc-extension-v4_epoch0.pth\n",
            "19:49:22 Finished loading optimizer state_dict from ./drive/MyDrive/Projects/FNC/Models/fnc-extension-v4-albert-tweets-2020-01-22/fnc-extension-v4_epoch0_training.pth\n",
            "19:49:22 Finished loading scheduler state_dict from ./drive/MyDrive/Projects/FNC/Models/fnc-extension-v4-albert-tweets-2020-01-22/fnc-extension-v4_epoch0_training.pth\n",
            "19:49:22 Finished loading loss state_dict from ./drive/MyDrive/Projects/FNC/Models/fnc-extension-v4-albert-tweets-2020-01-22/fnc-extension-v4_epoch0_training.pth\n",
            "19:49:22 Performing initial evaluation...\n",
            "19:50:01 \t\tReconstruction\tDomain 0: 0.041\n",
            "19:50:01 \tMasked Prediction\tDomain 0: 0.041\n",
            "19:50:01 \tUnmasked Prediction\tDomain 0: 0.000\n",
            "19:50:01 Starting training from 1\n",
            "19:50:01 Parameter Group `opt-1`: Starting epoch 1 with 1100 steps and learning rate 1.00000E-05\n",
            "19:50:22 Epoch1.99\tMaskedLM: 8.406\tReconstruct: 0.079\tMasked Acc: 0.043\n",
            "19:50:46 Epoch1.199\tMaskedLM: 8.447\tReconstruct: 0.083\tMasked Acc: 0.048\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-278900bcb60f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/GLAMOR/src/ednaml/core/EdnaML.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontinue_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprevious_stop\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/GLAMOR/src/ednaml/trainer/BaseTrainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, continue_epoch)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mcontinue_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;31m# TODO pre epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m                 \u001b[0;31m# TODO post epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/GLAMOR/src/ednaml/trainer/BaseTrainer.py\u001b[0m in \u001b[0;36mepoch_step\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#train == we are tracking all numbers and computation graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#perform function and returns loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/GLAMOR/profiles/FNC/fnc-extension.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m         (\n\u001b[1;32m    182\u001b[0m             \u001b[0mall_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/GLAMOR/profiles/FNC/fnc-extension.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m         (\n\u001b[1;32m    182\u001b[0m             \u001b[0mall_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/GLAMOR/src/ednaml/trainer/BaseTrainer.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmove_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "eml.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iOQfJhKsYZi",
        "outputId": "941e8c7b-6375-497d-fbf4-c3a807b7386f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "19:51:04 Saving model, optimizer, and scheduler.\n",
            "19:51:05 Performing drive backup of model, optimizer, and scheduler.\n"
          ]
        }
      ],
      "source": [
        "eml.trainer.save()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rfjHQ0Osg5c"
      },
      "outputs": [],
      "source": [
        "eml.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBwOfPCRsuvq"
      },
      "source": [
        "## 4.1.1. Deployment to generate training features\n",
        "Now we will use our trained LLM to generate the training features again.\n",
        "\n",
        "These features will be used to determine the best number of clusters and other hyperparameters. Subsequently, we can use our calculated hyperparameters to build a K-Means Clustering Plugin that includes a High-Density Set estimator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S10EGxFMkQWR"
      },
      "outputs": [],
      "source": [
        "! rm *.h5*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZBTqabVizdV"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJCi3pnLtDMk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import ednaml\n",
        "import glob, os\n",
        "from ednaml.core import EdnaDeploy, EdnaML\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9T1XbZd0DOrO"
      },
      "outputs": [],
      "source": [
        "config = \"./GLAMOR/profiles/FNC/extension/config.yml\"\n",
        "deploy = \"./GLAMOR/profiles/FNC/extension/training_features.yml\"  # remove masking in data generation process\n",
        "crawler = \"./GLAMOR/profiles/FNC/fnc-crawler.py\"\n",
        "filtermask_generator = \"./GLAMOR/profiles/FNC/fnc-filtermasked.py\"\n",
        "extension_model = \"./GLAMOR/profiles/FNC/fnc-extension.py\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovIrGkTDtFiu"
      },
      "outputs": [],
      "source": [
        "ed = EdnaDeploy(config=config, deploy = deploy, dataloader_mode = \"train\", config_inject=[(\"SAVE.MODEL_QUALIFIER\", tweet_file)])\n",
        "ed.cfg.DEPLOYMENT.OUTPUT_ARGS[\"feature_file\"] = \"%s-filtered-features\"%tweet_file\n",
        "ed.cfg.DEPLOYMENT.DATAREADER.CRAWLER_ARGS[\"train_file\"] = \"%s-filtered.json\"%tweet_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jM7Ao46ZtFc1"
      },
      "outputs": [],
      "source": [
        "ed.add(crawler)\n",
        "ed.add(filtermask_generator)\n",
        "ed.add(extension_model)  # With model AND deployment defined\n",
        "ed.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLotDqjQtFQg"
      },
      "outputs": [],
      "source": [
        "ed.deploy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBH_VHmGAqbX"
      },
      "source": [
        "## 4.2 Estimating best number of clusters from the BERT features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLHxGtD9JPDZ"
      },
      "outputs": [],
      "source": [
        "def cluster_sweep(features_file, batch_size, max_iters, cluster_range):\n",
        "  import h5py, time\n",
        "  from sklearn.cluster import MiniBatchKMeans\n",
        "  inertia = []\n",
        "  data = h5py.File(features_file, 'r')\n",
        "  data_size = data['features'].shape[0]\n",
        "  \n",
        "  for k in cluster_range:\n",
        "    print(\"Starting sweep for k={kval}, with {iters} iterations\".format(kval=k, iters=max_iters))\n",
        "    kmeans = MiniBatchKMeans(n_clusters = k, random_state = 23465356, batch_size = batch_size)\n",
        "    stime = time.time()\n",
        "    for iters in range(max_iters):\n",
        "      for i in range(0, data_size, batch_size):\n",
        "          current_data = data['features'][i:i+batch_size]\n",
        "          kmeans.partial_fit(current_data)\n",
        "      if iters%5 == 0:\n",
        "        etime = round(time.time() - stime, 2)\n",
        "        print(\"\\t[{elapse} s] -- Completed {iters} iterations\".format(iters=iters, elapse = etime))\n",
        "        stime = time.time()\n",
        "    print(\"\\tCompeted MBKM for k={kval}, with inertia: {inertia}\".format(kval=k, inertia = kmeans.inertia_))\n",
        "    inertia.append(kmeans.inertia_)\n",
        "  return inertia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnHgqBPoJPBB"
      },
      "outputs": [],
      "source": [
        "cluster_range = [10,20,50]\n",
        "inertia = cluster_sweep(\n",
        "    features_file = \"%s-filtered-features.h5\"%tweet_file,\n",
        "    batch_size=256,\n",
        "    max_iters = 25,\n",
        "    cluster_range = cluster_range\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYUA2w_dI7fd"
      },
      "outputs": [],
      "source": [
        "# In case of interruptions or crashes...\n",
        "#inertia2 = [1660, 5510, 3100, 2290] + inertia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CArQNFjZRzT"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "sns.set_theme(style=\"darkgrid\")\n",
        "sns.lineplot(x=\"cluster_range\", y=\"inertia\", data=pd.DataFrame(list(zip(inertia, cluster_range)), columns=[\"inertia\", \"cluster_range\"]), \n",
        "             palette=\"tab10\", linewidth=2.5, marker=\"o\", markersize=10). \\\n",
        "              set(title=\"Elbow Plot for KMeans\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3fIovF-rCHb"
      },
      "source": [
        "So, we find here that for FNC Jan 2020 that **20 PROXIES** fits the elbow visual test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ngojQFRWcf9"
      },
      "outputs": [],
      "source": [
        "proxy = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EU53fUFKuFEb"
      },
      "source": [
        "## 4.3 Generate Plugins \n",
        "\n",
        "Using k=20 proxies, we will now generate the FastKMeans Plugin for our LLM for this FNC window. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tc1Iz_XU84Ia"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDroKNDP84Ia"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import ednaml\n",
        "import glob, os\n",
        "from ednaml.core import EdnaDeploy, EdnaML\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3kmromk84Ib"
      },
      "outputs": [],
      "source": [
        "config = \"./GLAMOR/profiles/FNC/extension/config.yml\"\n",
        "deploy = \"./GLAMOR/profiles/FNC/extension/fastkmeans.yml\" #basically, set up plugin info, and remove masking\n",
        "crawler = \"./GLAMOR/profiles/FNC/fnc-crawler.py\"\n",
        "filtermask_generator = \"./GLAMOR/profiles/FNC/fnc-filtermasked.py\"\n",
        "extension_model = \"./GLAMOR/profiles/FNC/fnc-extension.py\"\n",
        "fastkmeans = \"./GLAMOR/profiles/FNC/fnc-fastkmeans.py\"  # contains plugin and deployment for the plugin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5vnq4uJ84Ib"
      },
      "outputs": [],
      "source": [
        "ed = EdnaDeploy(config=config, deploy = deploy, dataloader_mode = \"train\", config_inject=[(\"SAVE.MODEL_QUALIFIER\", tweet_file), (\"SAVE.SAVE_FREQUENCY\", 1)])\n",
        "ed.cfg.DEPLOYMENT.DATAREADER.CRAWLER_ARGS[\"train_file\"] = \"%s-filtered.json\"%tweet_file\n",
        "ed.cfg.MODEL_PLUGIN[\"FastKMP-l2\"].PLUGIN_KWARGS[\"proxies\"] = proxy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmlZV-sy84Ib"
      },
      "outputs": [],
      "source": [
        "ed.add(crawler)\n",
        "ed.add(filtermask_generator)\n",
        "ed.add(extension_model)  # With model AND deployment defined\n",
        "ed.add(fastkmeans)  # # With plugin AND deployment defined; deployment replaces prior\n",
        "ed.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UD7rmlAG84Ib"
      },
      "outputs": [],
      "source": [
        "ed.deploy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WETqeAOTuoGl"
      },
      "source": [
        "## 4.4 Apply Plugin to Generate Neighbors\n",
        "\n",
        "Now we will use our generated plugin to determine which members of fns-unfiltered should belong in fnc-filtered because they are neighbors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dw555d1N90KW"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Df3g-6C590KX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import ednaml\n",
        "import glob, os\n",
        "from ednaml.core import EdnaDeploy, EdnaML\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ax2DiUM90KX"
      },
      "outputs": [],
      "source": [
        "config = \"./GLAMOR/profiles/FNC/extension/config.yml\"\n",
        "deploy = \"./GLAMOR/profiles/FNC/extension/neighbors.yml\" #basically, set up plugin info, and remove masking\n",
        "crawler = \"./GLAMOR/profiles/FNC/fnc-crawler.py\"\n",
        "filtermask_generator = \"./GLAMOR/profiles/FNC/fnc-filtermasked.py\"\n",
        "extension_model = \"./GLAMOR/profiles/FNC/fnc-extension.py\"\n",
        "fastkmeans = \"./GLAMOR/profiles/FNC/fnc-fastkmeans.py\"  # contains plugin and deployment for the plugin\n",
        "neighbors = \"./GLAMOR/profiles/FNC/fnc-neighbors.py\"  # contains deployment for neighbors generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWGSD2pG90KX"
      },
      "outputs": [],
      "source": [
        "ed = EdnaDeploy(config=config, deploy = deploy, config_inject=[(\"SAVE.MODEL_QUALIFIER\", tweet_file), (\"SAVE.DRIVE_BACKUP\", True)])\n",
        "ed.cfg.DEPLOYMENT.OUTPUT_ARGS[\"basename\"] = \"%s\"%tweet_file  # deployment will generate twt-neighbors.json\n",
        "ed.cfg.DEPLOYMENT.DATAREADER.CRAWLER_ARGS[\"test_file\"] = \"%s-unfiltered.json\"%tweet_file\n",
        "ed.cfg.MODEL_PLUGIN[\"FastKMP-l2\"].PLUGIN_KWARGS[\"proxies\"] = proxy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGVhF39190KX"
      },
      "outputs": [],
      "source": [
        "ed.add(crawler)\n",
        "ed.add(filtermask_generator)\n",
        "ed.add(extension_model)  # With model AND deployment defined\n",
        "ed.add(fastkmeans)  # # With plugin AND deployment defined; deployment replaces prior\n",
        "ed.add(neighbors)\n",
        "ed.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mnu-Q77o90KX"
      },
      "outputs": [],
      "source": [
        "ed.deploy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rmrRbkmCzBW"
      },
      "source": [
        "## 4.5. Generating FNC-Extended = FNC-Filtered + FNC-Neighbors\n",
        "\n",
        "Here, we combine fnc-filtered.json and fnc-neighbors.json to create fnc-extended.json. Then we upload this to our azure blob for this month!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6duv7zdvBR5"
      },
      "outputs": [],
      "source": [
        "original = sum(1 for line in open(\"%s.json\"%tweet_file))\n",
        "filtered = sum(1 for line in open(\"%s-filtered.json\"%tweet_file))\n",
        "unfiltered = sum(1 for line in open(\"%s-unfiltered.json\"%tweet_file))\n",
        "neighbor = sum(1 for line in open(\"%s-neighbor.json\"%tweet_file))\n",
        "extension = 100 * neighbor / filtered\n",
        "lift = 100 * neighbor / unfiltered\n",
        "print(\n",
        "\"\"\"\n",
        "-------- STATISTICS ------------\n",
        "\n",
        "ORIGINAL    {original}\n",
        "FILTERED    {filtered}\n",
        "UNFILTERED  {unfiltered}\n",
        "NEIGHBORS   {neighbor}\n",
        "\n",
        "EXTENSION   {extension}%\n",
        "LIFT        {lift}%\n",
        "\"\"\".format(original=original, filtered=filtered, unfiltered=unfiltered, neighbor=neighbor, extension=round(extension,2), lift=round(lift,2)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTCXrZ8ZBDYA"
      },
      "outputs": [],
      "source": [
        "cat \"$tweet_file-filtered.json\" \"$tweet_file-neighbor.json\" >> \"$tweet_file-extended.json\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMcmognzvBq2"
      },
      "source": [
        "## 4.6. Generate FNC-Extended-Oracle-Unlabeled, and upload\n",
        "\n",
        "Here, we will randomly select ~500 points to perform manual labeling. We will try to select these points from each cluster of our FNC LLM Model.\n",
        "\n",
        "Specifically, we will first use our existing plugin to compute the distances of each point in the extension to its cluster center, as well as its cluster index (the index itself is not semantic). \n",
        "\n",
        "After the deployment is complete, we will sweep across the data to find, for each cluster, the closest point, as well as some points further away to generate a more complete representation. That is, since we have 20 clusters (in FNC Jan), and require 500 points, for each cluster we can bin  into 24 regions + closest, and from the 24 regions, select one point at random."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipXmrO-NEmVj"
      },
      "outputs": [],
      "source": [
        "#!wget https://ednadatasets.blob.core.windows.net/edna-covid-extended/$tweet_file-extended.json.gz\n",
        "#!gunzip $tweet_file-extended.json.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dj8EL-WQNMLk"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mgUBMCkNMLl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import ednaml\n",
        "import glob, os\n",
        "from ednaml.core import EdnaDeploy, EdnaML\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfoqPP8ONMLl"
      },
      "outputs": [],
      "source": [
        "config = \"./GLAMOR/profiles/FNC/extension/config.yml\"\n",
        "deploy = \"./GLAMOR/profiles/FNC/extension/oracle.yml\" #basically, set up plugin info, and remove masking; shuffle, and set up the inputs for oracle file generation\n",
        "crawler = \"./GLAMOR/profiles/FNC/fnc-crawler.py\"\n",
        "filtermask_generator = \"./GLAMOR/profiles/FNC/fnc-filtermasked.py\"\n",
        "extension_model = \"./GLAMOR/profiles/FNC/fnc-extension.py\"\n",
        "fastkmeans = \"./GLAMOR/profiles/FNC/fnc-fastkmeans.py\"  # contains plugin and deployment for the plugin\n",
        "oracle = \"./GLAMOR/profiles/FNC/fnc-oracle.py\"  # contains deployment for oracle selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RNyuhBVNMLl"
      },
      "outputs": [],
      "source": [
        "ed = EdnaDeploy(config=config, deploy = deploy, config_inject=[(\"SAVE.MODEL_QUALIFIER\", tweet_file), (\"SAVE.DRIVE_BACKUP\", True)])\n",
        "ed.cfg.DEPLOYMENT.OUTPUT_ARGS[\"basename\"] = \"%s\"%tweet_file  \n",
        "ed.cfg.DEPLOYMENT.DATAREADER.CRAWLER_ARGS[\"test_file\"] = \"%s-extended.json\"%tweet_file\n",
        "ed.cfg.MODEL_PLUGIN[\"FastKMP-l2\"].PLUGIN_KWARGS[\"proxies\"] = proxy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjJqrMnZNMLl"
      },
      "outputs": [],
      "source": [
        "ed.add(crawler)\n",
        "ed.add(filtermask_generator)\n",
        "ed.add(extension_model)  # With model AND deployment defined\n",
        "ed.add(fastkmeans)  # # With plugin AND deployment defined; deployment replaces prior\n",
        "ed.add(oracle)\n",
        "ed.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmtVr6ljwF6-"
      },
      "outputs": [],
      "source": [
        "ed.deploy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5ADOWKGvn4Y"
      },
      "source": [
        "## 4.7 Upload to azure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bPh8p_h31nz"
      },
      "source": [
        "https://docs.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-python?tabs=environment-variable-windows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aou8Ri3YQ-Tb"
      },
      "outputs": [],
      "source": [
        "!gzip $tweet_file-oracle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzltNT_GoiWh"
      },
      "outputs": [],
      "source": [
        "!gunzip $tweet_file-extended.json.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DEardAhSiAW"
      },
      "outputs": [],
      "source": [
        "!gzip $tweet_file-extended.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFjGRDua2SSJ"
      },
      "outputs": [],
      "source": [
        "!pip install azure-storage-blob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJ29pqNr3Edi"
      },
      "outputs": [],
      "source": [
        "import os, uuid\n",
        "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient, __version__\n",
        "\n",
        "print(\"Azure Blob Storage v\" + __version__ + \" - Python quickstart sample\")\n",
        "connect_str = \"DefaultEndpointsProtocol=https;AccountName=ednadatasets;AccountKey=7pZKgsFLvckW3933BSOIzRvarSisKeDSCT5E/dXNs7vlfEEHG5MworeL2VoyA14pcZXoCaHNIR5r+AStqZQGaQ==;EndpointSuffix=core.windows.net\"\n",
        "# Create the BlobServiceClient object which will be used to create a container client\n",
        "blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
        "\n",
        "extended_file = \"%s-extended.json.gz\"%tweet_file\n",
        "oracle_file = \"%s-oracle.json.gz\"%tweet_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6eYqvpq5fKD"
      },
      "outputs": [],
      "source": [
        "# Create a unique name for the container\n",
        "container_name = \"edna-covid-extended\"\n",
        "blob_client = blob_service_client.get_blob_client(container=container_name, blob=extended_file)\n",
        "with open(extended_file, \"rb\") as data:\n",
        "    blob_client.upload_blob(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZ1GG6he5cuf"
      },
      "outputs": [],
      "source": [
        "# Create a unique name for the container\n",
        "container_name = \"edna-covid-extended-oracle\"\n",
        "blob_client = blob_service_client.get_blob_client(container=container_name, blob=oracle_file)\n",
        "with open(oracle_file, \"rb\") as data:\n",
        "    blob_client.upload_blob(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Generate Labels from Experts\n",
        "\n",
        "Here, we will label FNC-Extended for a month with models from ENFD, Fakeddit, and NELA's Bert-variant models. For now, we will use the Albert-Base-v2 models only.\n",
        "\n",
        "For each classifier, we will generate a text file with labels for each entry. Since there is no data shuffling, labels across text files will sync up. We will then upload these saved files to our Drive location for ease of access and future integration.\n",
        "\n",
        "We need the following outputs in each text file:\n",
        "  - **Predicted label.** 0 for True news, 1 for Fake news.\n",
        "  - **Distance-to-proxy (l2)**. Raw distance in embedding space to the nearest proxy.\n",
        "  - **Distance-to-proxy (cos)**. Cosine distance\n",
        "  - **High density set**. 0 or 1. Whether sample is in the high density set.\n",
        "  - **High density label**. The ground truth label of the corresponding high-density set cluster. This is the same as the proxy label, or nearest proxy label.\n",
        "  - **L-Score**. Smoothness score for the input using perturbations. Probably take the longest time.\n",
        "  - **L-Threshold**. The threshold for the nearest proxy / high density set.\n",
        "  - **Logit-confidence**. The raw logit probability of the predicted class.\n",
        "  - **Logit-average**. The average logit probability of the predicted class."
      ],
      "metadata": {
        "id": "I45khl9Bum_Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Iav_55EjkC4"
      },
      "outputs": [],
      "source": [
        "# See FNC Experts (And Masking!) notebook\n",
        "tweet_file = \"tweets-2020-01-22\"\n",
        "\n",
        "# efnd | nela | fakeddit\n",
        "dataset = \"efnd\"    \n",
        "\n",
        "# (ONLY FOR efnd; OTHERS YOU CAN IGNORE) \n",
        "# cmu_miscov19 | kagglefn_short | kagglefn_long | cov19_fn_title | cov19_fn_text | coaid_news | cov_rumor | covid_fn | covid_cq\n",
        "subdataset = \"kagglefn_short\"\n",
        "\n",
        "# nomask | rtm | rwm | ktm_tfidf | kwm_tfidf | ktm_att | kwm_att | ktrtm_tfidf | kwrwm_tfidf | ktrtm_att | kwrwm_att\n",
        "masking = \"nomask\"  \n",
        "\n",
        "# albert-base-v2\n",
        "model = \"albert-base-v2\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applying"
      ],
      "metadata": {
        "id": "v0rKQQ1_5AHa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfNxLlMNYNFU"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Select the correct model-file!\n",
        "model_variant = \"./GLAMOR/profiles/FNC/experts/configs/model-bertvariant.yml\" \n",
        "\n",
        "# plugin deploy file and proxies\n",
        "plugin_deploy_generation_file = \"./GLAMOR/profiles/FNC/experts/plugin_generation.py\" \n",
        "plugin_proxies = {\"efnd-cmu_miscov19\": 20, \"efnd-kagglefn_short\": 40, \"efnd-kagglefn_long\": 20,\n",
        "                  \"efnd-cov19_fn_title\": 30, \"efnd-cov19_fn_text\": 35, \"efnd-coaid_news\": 50,\n",
        "                  \"efnd-cov_rumor\": 30, \"efnd-covid_fn\": 30, \"efnd-covid_cq\": 50, \n",
        "                  \"nela\": 40, \"fakeddit\": 30}\n",
        "if dataset == \"efnd\":\n",
        "  proxy = plugin_proxies[dataset + \"-\" + subdataset]\n",
        "else:\n",
        "  proxy = plugin_proxies[dataset]\n",
        "\n",
        "if dataset == \"efnd\":\n",
        "  #----------------------------- EFND ------------------------------------\n",
        "  dataset_args = {\n",
        "      \"data_folder\" : \"Data\",\n",
        "      \"include\": [subdataset]\n",
        "  }\n",
        "  model_qualifier = subdataset\n",
        "elif dataset == \"nela\":\n",
        "  #----------------------------- NELA ------------------------------------\n",
        "  dataset_args = {\n",
        "      \"data_folder\" : \"Data\",\n",
        "      \"sub_folder\" : \"nela-covid-2020\"\n",
        "  }\n",
        "  model_qualifier = \"nela_covid_2020\"\n",
        "elif dataset == \"fakeddit\":\n",
        "  #----------------------------- FAKEDDIT ------------------------------------\n",
        "  dataset_args = {\n",
        "      \"data_folder\" : \"Data\"\n",
        "  }\n",
        "  model_qualifier = \"fakeddit\"\n",
        "else:\n",
        "  raise NotImplementedError()\n",
        "\n",
        "# nomask | rtm | rwm | ktm_tfidf | kwm_tfidf | ktm_att | kwm_att | ktrtm_tfidf | kwrwm_tfidf | ktrtm_att | kwrwm_att\n",
        "mask_overall = True;tm = False;wm = False;ktm=False;kwm=False;\n",
        "if masking == \"nomask\":\n",
        "  mask_overall = False\n",
        "if masking == \"rtm\" or masking == \"ktrtm_tfidf\" or masking == \"ktrtm_att\":\n",
        "  tm = True\n",
        "if masking == \"rwm\" or masking == \"kwrwm_tfidf\" or masking == \"kwrwm_att\":\n",
        "  wm = True\n",
        "if masking == \"ktm_tfidf\" or masking == \"ktm_att\" or masking == \"ktrtm_att\":\n",
        "  ktm = True\n",
        "if masking == \"kwm_tfidf\" or masking == \"kwm_att\" or masking == \"kwrwm_att\":\n",
        "  kwm = True\n",
        "\n",
        "# Other options\n",
        "model_core_name = \"-\".join([\"fnc\",\"expert\",dataset, masking])\n",
        "model_backbone = model\n",
        "model_base = model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3K40ykXsESB3"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ednaml, torch\n",
        "from ednaml.core import EdnaDeploy"
      ],
      "metadata": {
        "id": "N2TvORAG_25Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ednaml.deploy.HFDeploy import HFDeploy\n",
        "import os\n",
        "\n",
        "class LabelingDeployment(HFDeploy):\n",
        "  def output_setup(self, **kwargs):\n",
        "    self.ofile = kwargs.get(\"label_file_name\")\n",
        "    if self.ofile == \"None\":\n",
        "      raise ValueError(\"No filename provided\")\n",
        "    self.oobj = open(self.ofile, \"w\")\n",
        "    self.oobj.write(\",\".join([\n",
        "      \"predicted_label\",\n",
        "      \"l2_dist\",\n",
        "      \"l2_hdthreshold\",\n",
        "      \"l2_proxylabels\",\n",
        "      \"cos_dist\",\n",
        "      \"cos_hdthreshold\",\n",
        "      \"cos_proxylabels\",\n",
        "      \"l_score\",\n",
        "      \"smooth_l_score\",\n",
        "      \"l_threshold\",\n",
        "      \"smooth_l_threshold\",\n",
        "      \"l_proxylabel\",\n",
        "      \"logit_raw\",\n",
        "      \"logit_threshold\"\n",
        "      ])+\"\\n\")\n",
        "\n",
        "  def output_step(self, logits, features, secondary):\n",
        "    predicted_label = torch.argmax(torch.nn.functional.softmax(logits.cpu(), dim=1), dim=1).tolist()\n",
        "    l2_dist = secondary[2][\"FastKMP-l2\"][\"distance\"].to(torch.float32).tolist()\n",
        "    l2_hdthreshold = secondary[2][\"FastKMP-l2\"][\"threshold\"].to(torch.float32).tolist()\n",
        "    l2_proxylabels = secondary[2][\"FastKMP-l2\"][\"proxy_labels\"].to(torch.float32).tolist()\n",
        "    \n",
        "    cos_dist = secondary[2][\"FastKMP-cos\"][\"distance\"].to(torch.float32).tolist()\n",
        "    cos_hdthreshold = secondary[2][\"FastKMP-cos\"][\"threshold\"].to(torch.float32).tolist()\n",
        "    cos_proxylabels = secondary[2][\"FastKMP-cos\"][\"proxy_labels\"].to(torch.float32).tolist()\n",
        "\n",
        "    l_score = secondary[2][\"FRL-midas\"][\"l_score\"]\n",
        "    smooth_l_score = secondary[2][\"FRL-midas\"][\"smooth_l_score\"]\n",
        "    l_threshold = secondary[2][\"FRL-midas\"][\"l_threshold\"]\n",
        "    smooth_l_threshold = secondary[2][\"FRL-midas\"][\"smooth_l_threshold\"]\n",
        "    l_proxylabel = secondary[2][\"FRL-midas\"][\"proxy_label\"].to(torch.float32).tolist()\n",
        "\n",
        "    logit_raw = secondary[2][\"logit-confidence\"][\"logit\"].to(torch.float32).tolist()\n",
        "    logit_threshold = secondary[2][\"logit-confidence\"][\"logit_threshold\"].to(torch.float32).tolist()\n",
        "\n",
        "    output_list = [\",\".join(map(str,item)) for item in zip(\n",
        "      predicted_label,\n",
        "      l2_dist,\n",
        "      l2_hdthreshold,\n",
        "      l2_proxylabels,\n",
        "      cos_dist,\n",
        "      cos_hdthreshold,\n",
        "      cos_proxylabels,\n",
        "      l_score,\n",
        "      smooth_l_score,\n",
        "      l_threshold,\n",
        "      smooth_l_threshold,\n",
        "      l_proxylabel,\n",
        "      logit_raw,\n",
        "      logit_threshold\n",
        "      )]\n",
        "    self.oobj.write(\"\\n\".join(output_list)+\"\\n\")\n",
        "  def end_of_deployment(self):\n",
        "      self.oobj.close()"
      ],
      "metadata": {
        "id": "MLOF-jQ-KIbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ed = EdnaDeploy(config=[\"./GLAMOR/profiles/FNC/labeling/config_base.yml\",  # logging, deployment, crawler\n",
        "                          model_variant,\n",
        "                        \"./GLAMOR/profiles/FNC/experts/configs/plugin_base.yml\"],\n",
        "                config_inject = [\n",
        "                    (\"SAVE.MODEL_CORE_NAME\", model_core_name),\n",
        "                    (\"SAVE.MODEL_BACKBONE\", model_backbone),\n",
        "                    (\"SAVE.MODEL_QUALIFIER\", model_qualifier),\n",
        "                    (\"SAVE.DRIVE_BACKUP\", True),\n",
        "                    (\"SAVE.LOG_BACKUP\", False),\n",
        "                    (\"MODEL.MODEL_BASE\", model_backbone),\n",
        "                    (\"SAVE.SAVE_FREQUENCY\", 5), # To ensure nothing actually gets saved, since we only run for 1 epoch\n",
        "                    (\"DEPLOYMENT.EPOCHS\", 1)\n",
        "                ])\n",
        "\n",
        "ed.cfg.DEPLOYMENT.OUTPUT_ARGS[\"label_file_name\"] = tweet_file + \"-\" + ed.saveMetadata.MODEL_SAVE_FOLDER+\".csv\"\n",
        "ed.cfg.DEPLOYMENT.DATAREADER.CRAWLER_ARGS[\"azfile\"] = \"%s-extended.json.gz\"%tweet_file\n",
        "# No masking for everything, so we will not edit it here. \n",
        "\n",
        "\n",
        "ed.add(\"./GLAMOR/profiles/FNC/fnc-extended-labeling-crawler.py\")\n",
        "ed.addDeploymentClass(LabelingDeployment)"
      ],
      "metadata": {
        "id": "Y4cCp18M_22i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ed.apply()"
      ],
      "metadata": {
        "id": "k6nKa_ixKEOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ed.deploy()"
      ],
      "metadata": {
        "id": "cZhe3AohKFJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ed.model.plugins[\"FastKMP-l2\"].proxies"
      ],
      "metadata": {
        "id": "Z-FHK2N8ebDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv(\"tweets-2020-01-22-fnc-expert-efnd-nomask-v1-albert-base-v2-kagglefn_short.csv\")"
      ],
      "metadata": {
        "id": "OU0F_yBBffb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "E7zQy8x4fkbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring"
      ],
      "metadata": {
        "id": "6xm3Kuc-NDKa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdDVnQvLkaAi"
      },
      "source": [
        "# ------------------------   Extras   -----------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxY59vsPkb49"
      },
      "source": [
        "# Generating plugins"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfXWZ4FNmzGx"
      },
      "source": [
        "### CHECKLIST:\n",
        "\n",
        "- is the `model_config` correct?\n",
        "- is the `deploy_config` correct?\n",
        "- is the `model_plugins` correct? This should not need to change for any of them.\n",
        "\n",
        "- is the `model_functions` correct? Cross-reference to `model_config`!\n",
        "\n",
        "- is the `dataloader_mode` correct?\n",
        "- is the `batch_size` correct?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnunIsv5hGVf"
      },
      "outputs": [],
      "source": [
        "GENERATING_PLUGINS = True\n",
        "model_config = \"./GLAMOR/profiles/MiDAS/expert-experiments/midas-cmumiscov-2.yml\"\n",
        "deploy_config = \"./GLAMOR/profiles/NELA/nela-covid-deploy.yml\"\n",
        "model_plugins = \"./GLAMOR/profiles/NELA/plugins.yml\"\n",
        "model_functions = [\"./GLAMOR/profiles/MiDAS/midas-dataset.py\",\n",
        "                    \"./GLAMOR/profiles/MiDAS/midas-expert.py\"]\n",
        "                    \n",
        "dataloader_mode = \"train\" # CHANGE THIS IF DEBUGGING\n",
        "batch_size = 128          # CHANGE THIS IF TOO SLOW OR CRASHING\n",
        "# EXTRAS\n",
        "deployment_functions = \"./GLAMOR/profiles/NELA/deploy-plugin-usage.py\"  # use generation.py is generating plugins\n",
        "epochs = 1                # CHANGE THIS IF GENERATING PLUGINS ----> 6       ELSE, use 1 epoch\n",
        "ignore_plugins = []       # CHANGE THIS IF PLUGINS NEED RESETTING ----> [\"KMP-l2\", \"KMP-cos\", \"RL-midas\"]\n",
        "if GENERATING_PLUGINS:\n",
        "  ignore_plugins = [\"KMP-l2\", \"KMP-cos\", \"RL-midas\"]\n",
        "  epochs = 6\n",
        "  deployment_functions =  \"./GLAMOR/profiles/NELA/deploy-plugin-generation.py\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HeB0G6csm9CW"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3lKPAzOm8_v"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import ednaml\n",
        "from ednaml.core import EdnaDeploy\n",
        "from ednaml.plugins.KMeansProxy import KMeansProxy\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGWP-Lzl5K86"
      },
      "outputs": [],
      "source": [
        "ed = EdnaDeploy(config=model_config, deploy=[deploy_config, model_plugins] , dataloader_mode = dataloader_mode)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zH2jXma3xhc3"
      },
      "source": [
        "We make sure we have the correct `DATAREADER`. Here we use the one from `EXECUTION`, because it comes from the model we are loading, ensuring we are working with the model's training/testing data. \n",
        "\n",
        "We can also manually set `EPOCHS` here, for debugging or fast deployment editing purposes. Since deployments are ephemeral and not logged (for the time being), this is sufficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sj2MR0k-XHOm"
      },
      "outputs": [],
      "source": [
        "ed.cfg.DEPLOYMENT.DATAREADER = ed.cfg.EXECUTION.DATAREADER\n",
        "#\n",
        "ed.cfg.DEPLOYMENT.DATAREADER.DATAREADER = \"AlbertReader\"\n",
        "ed.cfg.TEST_TRANSFORMATION.BATCH_SIZE = batch_size\n",
        "ed.cfg.DEPLOYMENT.EPOCHS = epochs\n",
        "ed.cfg.DEPLOYMENT.PLUGIN.HOOKS = 'warmup'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lG-X0lN3x5pa"
      },
      "source": [
        "If needed, go into the files and make sure the correct functions are registered with EdnaML, specifically the correct deployment plugin.\n",
        "\n",
        "There is one deployment plugin for writing to an output, and one for not writing to an output (for generating the plugins and doing nothing with the actual returns.\n",
        "\n",
        "**`NOTE: potential bug. midas-dataset also contains MiDASGenerator now, but this part NEEDS to use AlBERTReader. Solution. Deregister MiDASGenerator from midas-datasert. Solution: integrated MiDASGenerator and AlbertReader with datalabels.`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1wIwei15S04"
      },
      "outputs": [],
      "source": [
        "ed.add(model_functions)\n",
        "ed.add(deployment_functions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWfgOo9M3KGX"
      },
      "outputs": [],
      "source": [
        "ed.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQSP4Otj3UsV"
      },
      "outputs": [],
      "source": [
        "ed.deploy(ignore_plugins = ignore_plugins)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gUSjVY8ao8C"
      },
      "outputs": [],
      "source": [
        "ed.model.plugins['RL-midas'].save()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DArM05AeVeir"
      },
      "outputs": [],
      "source": [
        "ed.model.plugins['RL-midas'].save()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51AtEjJFl97w"
      },
      "source": [
        "# Cross Validation Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xHDwXl-3OTA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import ednaml\n",
        "import glob, os\n",
        "from ednaml.core import EdnaDeploy, EdnaML\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsZv09uPwHKb"
      },
      "source": [
        "## Testing NELA/Fakeddit on MiDAS Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yer5K3o9_Njq"
      },
      "outputs": [],
      "source": [
        "# NELA\n",
        "cfg = \"./GLAMOR/profiles/NELA/nela-covid-v1.yml\"\n",
        "model_file = \"./GLAMOR/profiles/NELA/nela.py\"\n",
        "# Fakeddit\n",
        "#cfg = \"./GLAMOR/profiles/Fakeddit/fakeddit-v3.yml\"\n",
        "#model_file = \"./GLAMOR/profiles/Fakeddit/fakeddit.py\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZ1znkUPwJ5z"
      },
      "outputs": [],
      "source": [
        "data_folder = \"Data\"\n",
        "datasets_folders = glob.glob(os.path.join(data_folder, \"*\"))\n",
        "datasets_folders = [os.path.basename(item) for item in datasets_folders if os.path.basename(item) not in [\"coaid_tweets\", \"recov\", \"nela-elections-2020\", \"all_train.tsv\", \"all_test_public.tsv\", \"nela-covid-2020\", \"nela-gt-2020\", \"all_validate.tsv\"]]\n",
        "datasets_folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFbzthaa2KI4"
      },
      "outputs": [],
      "source": [
        "# set up the model\n",
        "eml = EdnaML(config=cfg, mode=\"test\", test_only = True, logger_save_name=\"cd-eval\")\n",
        "eml2 = EdnaML(config=\"./GLAMOR/profiles/MiDAS/encoder-experiments/midas-tsne.yml\", \n",
        "                mode=\"test\", test_only = True, logger_save_name=\"cd-eval\")  # weights path should be automatically inferred!!! :)\n",
        "\n",
        "eml.cfg.EXECUTION.DATAREADER = eml2.cfg.EXECUTION.DATAREADER\n",
        "eml.cfg.EXECUTION.DATAREADER.DATAREADER = \"AlbertReader\"\n",
        "eml.cfg.EXECUTION.DATAREADER.CRAWLER_ARGS[\"include\"] = datasets_folders\n",
        "# So we have model loaded with correct parameters\n",
        "\n",
        "eml.add(model_file) #add model\n",
        "eml.add(\"./GLAMOR/profiles/MiDAS/midas-dataset.py\") # replace crawler\n",
        "eml.add(\"./GLAMOR/profiles/MiDAS/midas-trainer.py\") # replace trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vRzS3rI2KGY"
      },
      "outputs": [],
      "source": [
        "eml.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZNDv_rL-TnG"
      },
      "outputs": [],
      "source": [
        "preds, labels, classes, logits = eml.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "my3xebM87AcV"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "\n",
        "print (\"Domain accuracies for %s\"%str(eml.cfg.SAVE.MODEL_QUALIFIER))\n",
        "for idx, dlabel in enumerate(range(max(labels[1]).item()+1)):\n",
        "  acc = torch.mean((preds[labels[1]==dlabel] == labels[0][labels[1]==dlabel]).float())\n",
        "  micro_fscore = np.mean(f1_score(labels[0][labels[1]==dlabel], preds[labels[1]==dlabel], average=\"micro\"))\n",
        "  weighted_fscore = np.mean(f1_score(labels[0][labels[1]==dlabel], preds[labels[1]==dlabel], average=\"weighted\"))\n",
        "  print (\"\\t Domain {0}\\t {1:0.3f} \\t{2:0.3f} \\t{3:0.3f}\\t\\t{4}\".format(datasets_folders[idx], acc, micro_fscore, weighted_fscore, torch.sum(labels[1]==dlabel).item()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f--DYZ6S_dls"
      },
      "source": [
        "## Testing MIDAS Models on NELA/Fakeddit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZbcIvgDWf31"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import ednaml\n",
        "import glob, os\n",
        "from ednaml.core import EdnaDeploy, EdnaML\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFozzqfkAjb6"
      },
      "outputs": [],
      "source": [
        "expert_config = \"./GLAMOR/profiles/MiDAS/expert-experiments/midas-covrumor-1.yml\"\n",
        "\"\"\"\n",
        "\"./GLAMOR/profiles/MiDAS/expert-experiments/midas-kagglefnlong-1.yml\"\n",
        "\"./GLAMOR/profiles/MiDAS/expert-experiments/midas-covrumor-1.yml\"\n",
        "\"./GLAMOR/profiles/MiDAS/expert-experiments/midas-covidfn-1.yml\"\n",
        "\"./GLAMOR/profiles/MiDAS/expert-experiments/midas-covid_cq-1.yml\"\n",
        "\"./GLAMOR/profiles/MiDAS/expert-experiments/midas-cov19fntitle-1.yml\"\n",
        "\"./GLAMOR/profiles/MiDAS/expert-experiments/midas-cov19fntext-1.yml\"\n",
        "\"./GLAMOR/profiles/MiDAS/expert-experiments/midas-coaid_news-1.yml\"\n",
        "\"./GLAMOR/profiles/MiDAS/expert-experiments/midas-cmumiscov-2.yml\"\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHFm8FJa_tB-"
      },
      "outputs": [],
      "source": [
        "# NELA\n",
        "#cfg = \"./GLAMOR/profiles/NELA/nela-covid-v1.yml\"\n",
        "#crawler_file = \"./GLAMOR/profiles/NELA/nela.py\"\n",
        "# Fakeddit\n",
        "cfg = \"./GLAMOR/profiles/Fakeddit/fakeddit-v3.yml\"\n",
        "crawler_file = \"./GLAMOR/profiles/Fakeddit/fakeddit.py\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_msnPJpywL4z"
      },
      "outputs": [],
      "source": [
        "# set up the model\n",
        "eml = EdnaML(config=expert_config, mode=\"test\", test_only = True, logger_save_name=\"cd-eval\")\n",
        "eml2 = EdnaML(config=cfg, \n",
        "                mode=\"test\", test_only = True, logger_save_name=\"cd-eval\")  # weights path should be automatically inferred!!! :)\n",
        "\n",
        "eml.cfg.EXECUTION.DATAREADER = eml2.cfg.EXECUTION.DATAREADER\n",
        "# So we have model loaded with correct parameters\n",
        "\n",
        "eml.add(crawler_file) # add crawler/trainer\n",
        "eml.add(\"./GLAMOR/profiles/MiDAS/midas-expert.py\") #replace model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPa5Rg9yACYX"
      },
      "outputs": [],
      "source": [
        "eml.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTbhbcjUADOP"
      },
      "outputs": [],
      "source": [
        "preds, labels, classes, logits = eml.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tM_2VaBCWva"
      },
      "outputs": [],
      "source": [
        "labels[labels!=0] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_CSFxjPCWva"
      },
      "outputs": [],
      "source": [
        "accuracy = (preds != labels).sum().float() / float(labels.size(0))\n",
        "print(\"\\tAccuracy: {:.3%}\".format(accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMjNkF9q9Ihz"
      },
      "source": [
        "## NELA <-> Fakeddit cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08DGKH029K4S"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import ednaml\n",
        "import glob, os\n",
        "from ednaml.core import EdnaDeploy, EdnaML\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECXQSK4W9N6X"
      },
      "outputs": [],
      "source": [
        "# NELA\n",
        "b_cfg = \"./GLAMOR/profiles/NELA/nela-covid-v1.yml\"\n",
        "b_crawlerfile = \"./GLAMOR/profiles/NELA/nela.py\"\n",
        "# Fakeddit\n",
        "cfg = \"./GLAMOR/profiles/Fakeddit/fakeddit-v3.yml\"\n",
        "model_file = \"./GLAMOR/profiles/Fakeddit/fakeddit.py\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcQT02459UZW"
      },
      "outputs": [],
      "source": [
        "# set up the model\n",
        "eml = EdnaML(config=cfg, mode=\"test\", test_only = True, logger_save_name=\"cd-eval\")\n",
        "eml2 = EdnaML(config=b_cfg, \n",
        "                mode=\"test\", test_only = True, logger_save_name=\"cd-eval\")  # weights path should be automatically inferred!!! :)\n",
        "\n",
        "eml.cfg.EXECUTION.DATAREADER = eml2.cfg.EXECUTION.DATAREADER\n",
        "# So we have model loaded with correct parameters\n",
        "\n",
        "eml.add(b_crawlerfile) #add crawler/trainer\n",
        "eml.add(model_file) # replace model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHxhTQcW-E86"
      },
      "outputs": [],
      "source": [
        "eml.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dkk2B45j-EtK"
      },
      "outputs": [],
      "source": [
        "preds, labels, classes, logits = eml.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2YFkjvcgDFx"
      },
      "source": [
        "#### Converting the fakeddit true labels to other 1/0 versions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGsJx4o-BsRM"
      },
      "outputs": [],
      "source": [
        "labels[labels!=0] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nM35mp48CEW-"
      },
      "outputs": [],
      "source": [
        "accuracy = (preds == labels).sum().float() / float(labels.size(0))\n",
        "print(\"\\tAccuracy: {:.3%}\".format(accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_zzDtRf-oRj"
      },
      "outputs": [],
      "source": [
        "!rm -rf -- test-datashard-artifacts/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4t8TuXCxgGLe"
      },
      "source": [
        "#### Converting the Fakeddit predictions to other versions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKmXAGnP-qo3"
      },
      "outputs": [],
      "source": [
        "preds[preds!=0] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTv3_9DH_il5"
      },
      "outputs": [],
      "source": [
        "accuracy = (preds == labels).sum().float() / float(labels.size(0))\n",
        "print(\"\\tAccuracy: {:.3%}\".format(accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MGRfew7gMw0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "OJQ1vkUS-4hv",
        "OUETMOy3gGvu",
        "y4h8JjeAfpDo",
        "TqJoyEW1f_a1",
        "d5j3WfN0fpIT",
        "NQCbxSJDf7Ix",
        "0KjvT_ZJovGY",
        "FOwc42Dv5i6H",
        "bBLksdNI5Ukb",
        "kqSK-TBM5Z9v",
        "FBwOfPCRsuvq",
        "lBH_VHmGAqbX",
        "EU53fUFKuFEb",
        "WETqeAOTuoGl",
        "5rmrRbkmCzBW",
        "kMcmognzvBq2",
        "-5ADOWKGvn4Y",
        "I45khl9Bum_Y",
        "UdDVnQvLkaAi",
        "uxY59vsPkb49",
        "PfXWZ4FNmzGx",
        "51AtEjJFl97w",
        "f--DYZ6S_dls"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}